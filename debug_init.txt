Meta Info: {}
New Meta Info: {'micro_batch_size': 32, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 32, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 32, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 32, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 32, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 12692]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 12692]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 32, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 32, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 32, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 32, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 32, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 17185]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 17185]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 12616]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 12616]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 16758]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 16758]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 16686]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 16686]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 12757]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 12757]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 16689]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 16689]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 20692]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 20692]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([1312, 4096]) --- IGNORE ---
Input Size: torch.Size([1312, 256]) --- IGNORE ---
Output Size: torch.Size([1312, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([1312, 4096]) --- IGNORE ---
Input Size: torch.Size([1312, 256]) --- IGNORE ---
Output Size: torch.Size([1312, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([1312, 4096]) --- IGNORE ---
Input Size: torch.Size([1312, 256]) --- IGNORE ---
Output Size: torch.Size([1312, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([1312, 4096]) --- IGNORE ---
Input Size: torch.Size([1312, 256]) --- IGNORE ---
Output Size: torch.Size([1312, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([1312, 4096]) --- IGNORE ---
Input Size: torch.Size([1312, 256]) --- IGNORE ---
Output Size: torch.Size([1312, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([1312, 4096]) --- IGNORE ---
Input Size: torch.Size([1312, 256]) --- IGNORE ---
Output Size: torch.Size([1312, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([1312, 4096]) --- IGNORE ---
Input Size: torch.Size([1312, 256]) --- IGNORE ---
Output Size: torch.Size([1312, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
Length of Generation:Gen Size: torch.Size([1312, 4096]) --- IGNORE ---
Input Size: torch.Size([1312, 256]) --- IGNORE ---
Output Size: torch.Size([1312, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
New Meta Info: {}
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([1312, 4096]) --- IGNORE ---
Input Size: torch.Size([1312, 256]) --- IGNORE ---
Output Size: torch.Size([1312, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([1312, 4096]) --- IGNORE ---
Input Size: torch.Size([1312, 256]) --- IGNORE ---
Output Size: torch.Size([1312, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([1312, 4096]) --- IGNORE ---
Input Size: torch.Size([1312, 256]) --- IGNORE ---
Output Size: torch.Size([1312, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([1312, 4096]) --- IGNORE ---
Input Size: torch.Size([1312, 256]) --- IGNORE ---
Output Size: torch.Size([1312, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([128, 4096]) --- IGNORE ---
Input Size: torch.Size([128, 256]) --- IGNORE ---
Output Size: torch.Size([128, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([1312, 4096]) --- IGNORE ---
Input Size: torch.Size([1312, 256]) --- IGNORE ---
Output Size: torch.Size([1312, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([1312, 4096]) --- IGNORE ---
Input Size: torch.Size([1312, 256]) --- IGNORE ---
Output Size: torch.Size([1312, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 256]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 256]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {} --- IGNORE ---
Meta Info: {}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
New Meta Info: {'micro_batch_size': 16, 'max_token_len': 16384, 'use_dynamic_bsz': False, 'temperature': 1.0}
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
Length of Generation:Gen Size: torch.Size([64, 4096]) --- IGNORE ---
Input Size: torch.Size([64, 2048]) --- IGNORE ---
Output Size: torch.Size([64, 4096]) --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True} --- IGNORE ---
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
New Meta Info: {}
Meta Info: {'eos_token_id': 128009, 'pad_token_id': 128009, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
New Meta Info: {}
New Meta Info: {}
